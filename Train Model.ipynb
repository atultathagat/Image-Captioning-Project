{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input,Dropout,add,Embedding,Dense,LSTM\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Flickr8k_text/Flickr_8k.trainImages.txt','r') as file_obj:\n",
    "    data_train=file_obj.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Flickr8k_text/Flickr_8k.devImages.txt','r') as file_obj:\n",
    "    data_validate=file_obj.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image_captions_pkl', 'rb') as f:\n",
    "    img_captions_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image_features_pkl', 'rb') as f:\n",
    "    img_features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_pkl', 'rb') as f:\n",
    "    t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len=len(t.word_index)+1\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    y=[]\n",
    "    for cnt,i in enumerate(data):\n",
    "        img_name=i.split('.')[0]\n",
    "        captions=img_captions_dict[img_name]\n",
    "        features=img_features_dict[img_name]\n",
    "        for caption in captions:\n",
    "            words=caption.split()\n",
    "            for i in range(1,len(words)):\n",
    "                input_str=words[:i]\n",
    "                output_str=words[i]\n",
    "                x1.append(features)\n",
    "                x2.append(t.texts_to_sequences([input_str])[0])\n",
    "                y.append(t.texts_to_sequences([output_str])[0][0]) \n",
    "    x1=np.array(x1) \n",
    "    x2=pad_sequences(x2,maxlen=33)    \n",
    "    y=to_categorical(y,vocab_len)  \n",
    "    return x1,x2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train,x2_train,y_train=get_data(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305623, 4096)\n",
      "(305623, 33)\n",
      "(305623, 8362)\n"
     ]
    }
   ],
   "source": [
    "print(x1_train.shape)\n",
    "print(x2_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_val,x2_val,y_val=get_data(data_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51499, 4096)\n",
      "(51499, 33)\n",
      "(51499, 8362)\n"
     ]
    }
   ],
   "source": [
    "print(x1_val.shape)\n",
    "print(x2_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(33,))\n",
    "se1 = Embedding(vocab_len, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_len, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 256)      2140672     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 33, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8362)         2149034     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,929,642\n",
      "Trainable params: 5,929,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 305623 samples, validate on 51499 samples\n",
      "305623/305623 [==============================] - 1952s 6ms/sample - loss: 4.5218 - accuracy: 0.2370 - val_loss: 4.2099 - val_accuracy: 0.2702\n",
      "Train on 305623 samples, validate on 51499 samples\n",
      "305623/305623 [==============================] - 1878s 6ms/sample - loss: 3.8665 - accuracy: 0.2843 - val_loss: 4.0863 - val_accuracy: 0.2857\n",
      "Train on 305623 samples, validate on 51499 samples\n",
      "305623/305623 [==============================] - 1976s 6ms/sample - loss: 3.6586 - accuracy: 0.2973 - val_loss: 4.0623 - val_accuracy: 0.2905\n",
      "Train on 305623 samples, validate on 51499 samples\n",
      "305623/305623 [==============================] - 1886s 6ms/sample - loss: 3.5559 - accuracy: 0.3035 - val_loss: 4.0530 - val_accuracy: 0.2939\n",
      "Train on 305623 samples, validate on 51499 samples\n",
      " 67552/305623 [=====>........................] - ETA: 24:45 - loss: 3.4475 - accuracy: 0.3098"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for i in range(1,epochs+1):\n",
    "    model.fit([x1_train,x2_train],y_train,epochs=1,validation_data=([x1_val,x2_val],y_val))\n",
    "    model.save('model_image_captioning_'+str(i)+'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
